{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports and some basic stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Thu Aug 10 10:49:20 2017\n",
    "\n",
    "@author: Atif \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io as sio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for path and some initializations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelPath = 'savedFCNetModel/model.ckpt'\n",
    "summaryPath = 'FCNetlog/summaryLog'\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "\n",
    "tf.reset_default_graph()        \n",
    "init = tf.truncated_normal_initializer(stddev=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions\n",
    "### if you have some data in matlab, you can get it loaded in python environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadMatlabVar(matlabFileName, varName):\n",
    "    mat_contents = sio.loadmat(matlabFileName)\n",
    "    return mat_contents[varName]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer definations goes here \n",
    "i) Layers with weight initialization within the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Conv1D(x, nChannels, nameParam = 'conv', kernelSize = 3):\n",
    "    with tf.name_scope(nameParam):\n",
    "        # x is in format  [batch, length, channels]\n",
    "        # create weight var format [inChannel,outChannel, filters]\n",
    "        inChannels = x.get_shape()[2]\n",
    "        w = tf.get_variable(name=nameParam+'_w',shape=[kernelSize,inChannels,nChannels],dtype = tf.float32, initializer=init)\n",
    "        b = tf.get_variable(name=nameParam+'_b',dtype = tf.float32, initializer=tf.constant(0.01, shape=[nChannels], dtype=tf.float32))\n",
    "        return tf.nn.bias_add(tf.nn.conv1d(value = x,filters=w, stride=1,padding = 'VALID', name = nameParam),b)\n",
    "\n",
    "def batchNorm(x,nameParam='BN'):\n",
    "    # https://r2rt.com/implementing-batch-normalization-in-tensorflow.html\n",
    "    with tf.name_scope(nameParam):\n",
    "        inChannels = x.get_shape()[2]\n",
    "        batch_mean, batch_var = tf.nn.moments(x,[0])\n",
    "        #scale = tf.Variable(tf.ones([inChannels]))\n",
    "        #offset = tf.Variable(tf.zeros([inChannels]))\n",
    "        \n",
    "        offset  = tf.get_variable(name=nameParam+'_offset',shape=[inChannels],dtype = tf.float32, initializer=tf.zeros_initializer() )\n",
    "        scale  = tf.get_variable(name=nameParam+'_scale',shape=[inChannels],dtype = tf.float32, initializer= tf.ones_initializer() )\n",
    "        return tf.nn.batch_normalization(x,batch_mean, batch_var,offset,scale,0.01,name=nameParam)\n",
    "\n",
    "def LeakyReLU(x, nameParam='LeakyReLU'):\n",
    "    with tf.name_scope(nameParam):\n",
    "        alpha = tf.get_variable(name=nameParam+'_w',shape=[1],dtype = tf.float32, initializer=init)\n",
    "        return tf.nn.relu(x) - alpha * tf.nn.relu(-x)\n",
    "\n",
    "def FullyConnected(x,nodes,nameParam=\"fc\"):\n",
    "    with tf.name_scope(nameParam):\n",
    "        inNodes = x.get_shape()[1]\n",
    "        w = tf.get_variable(name=nameParam+'_w',shape=[inNodes,nodes],dtype = tf.float32, initializer=init)\n",
    "        b = tf.get_variable(name=nameParam+'_b',dtype = tf.float32, initializer=tf.constant(0.01, shape=[nodes], dtype=tf.float32))\n",
    "        fc = tf.nn.bias_add(tf.matmul(x,w),b,name = nameParam)\n",
    "        return fc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) Layers with weights as input to the layer. This is particualr useful when loadind the FCNet from the saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchNormWithWeights(x,offset,scale,nameParam='BN'):\n",
    "    with tf.name_scope(nameParam):\n",
    "        inChannels = x.get_shape()[2]\n",
    "        batch_mean, batch_var = tf.nn.moments(x,[0])\n",
    "        return tf.nn.batch_normalization(x,batch_mean,batch_var,offset,scale,0.01,name=nameParam)\n",
    "\n",
    "def Conv1DWithWeights(x, w, b, nChannels, nameParam = 'conv', kernelSize = 3):\n",
    "    with tf.name_scope(nameParam):\n",
    "        # x is in format  [batch, length, channels]    \n",
    "        return tf.nn.bias_add(tf.nn.conv1d(value = x,filters=w, stride=1,padding = 'VALID', name = nameParam),b)\n",
    "    \n",
    "def LeakyReLUWithWeights(x, alpha, nameParam='LeakyReLU'):\n",
    "    with tf.name_scope(nameParam):\n",
    "        #alpha = tf.get_variable(name=nameParam+'_w',shape=[1],dtype = tf.float32, initializer=init)\n",
    "        return tf.nn.relu(x) - alpha * tf.nn.relu(-x)\n",
    "def FullyConnectedWithWeights(x,w,b,nodes,nameParam=\"fc\"):\n",
    "    with tf.name_scope(nameParam):\n",
    "        fc = tf.nn.bias_add(tf.matmul(x,w),b,name = nameParam)\n",
    "        return fc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction of FCNet starts here\n",
    "### construction of a row. (please refer to the paper for details of the row in FCNet). This row is applied to the individual regional time-series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "x is a time series signal of fMRI data. In this model, its dimension is 172. So in tensorlfow its shape will be [None,172]\n",
    "'''\n",
    "def makeRow(x):\n",
    "        with tf.name_scope('FS_row'):\n",
    "            \n",
    "            x1 = Conv1D(x,32,'Conv1')\n",
    "            bn1 = batchNorm(x1,'BN1')\n",
    "            activ1 = LeakyReLU(bn1,'LReLU1')\n",
    "            pool1 = tf.layers.max_pooling1d(activ1,pool_size=2,strides=2, name = 'maxPool1')    \n",
    "            \n",
    "            c2 = Conv1D(pool1,64,'Conv2')\n",
    "            bn2 = batchNorm(c2,'BN2')\n",
    "            activ2 = LeakyReLU(bn2,'LReLU2')\n",
    "            pool2 = tf.layers.max_pooling1d(activ2,pool_size=2,strides=2, name = 'maxPool2')    \n",
    "            \n",
    "            c3 = Conv1D(pool2,96,'Conv3')\n",
    "            bn3 = batchNorm(c3,'BN3')\n",
    "            activ3 = LeakyReLU(bn3,'LReLU3')\n",
    "            \n",
    "            c4 = Conv1D(activ3,64,'Conv4')\n",
    "            c5 = Conv1D(c4,64,'Conv5')\n",
    "            \n",
    "            pool3 = tf.layers.max_pooling1d(c5,pool_size=2,strides=2, name = 'maxPool3')    \n",
    "            \n",
    "            #flattenSize = pool3.shape[1]*pool3.shape[2]\n",
    "            flatten = tf.reshape(pool3,[-1,17*64],name='flatten')\n",
    "            # dense 32        \n",
    "            fc = FullyConnected(flatten,32,'FC')\n",
    "            return fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The similarity measure network, that learns to caculate the similarity between the two pair of regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeDiffNN(tensor1,tensor2):\n",
    "    with tf.name_scope('DiffNN'):\n",
    "        merged = tf.concat([tensor1,tensor2],1,'merge')        \n",
    "        fc1 = FullyConnected(merged,32,'FC1')\n",
    "        fc2 = FullyConnected(fc1,32,'FC2')\n",
    "        predictions = FullyConnected(fc2,2,'predictions')\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following two functions are alike  makerow() and makeDiffNN above. The difference is that the below two functions are used if the FCNet is to be constructed from a saved model (graph). So these functions take graph as input and loads tensors from that graph and constructs the row and similarity measure network. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadRow(x,graph):\n",
    "    \n",
    "    with tf.name_scope('FS_row'):\n",
    "        w = graph.get_tensor_by_name('siameseNet/Conv1_w:0')\n",
    "        b = graph.get_tensor_by_name('siameseNet/Conv1_b:0')\n",
    "        wVal = w.eval()\n",
    "        bVal = b.eval()\n",
    "        wTensor = tf.get_variable('Conv1_w',initializer=tf.constant(wVal))\n",
    "        bTensor = tf.get_variable('Conv1_b',initializer=tf.constant(bVal))\n",
    "        x1 = Conv1DWithWeights(x, wTensor,bTensor,32,'Conv1')\n",
    "        #bn1 = tf.layers.batch_normal ization(x1,name='BN1')\n",
    "        \n",
    "        offset = graph.get_tensor_by_name('siameseNet/BN1_offset:0').eval()\n",
    "        scale = graph.get_tensor_by_name('siameseNet/BN1_scale:0').eval()\n",
    "        \n",
    "        bn1 = batchNormWithWeights(x1,offset,scale,'BN1')\n",
    "        #activ1 = tf.nn.relu(bn1)\n",
    "        alpha = graph.get_tensor_by_name('siameseNet/LReLU1_w:0')\n",
    "        alphaVal = alpha.eval()\n",
    "        alphaTensor = tf.get_variable('LReLU1_w',initializer=tf.constant(alphaVal))\n",
    "        activ1 = LeakyReLUWithWeights(bn1,alphaTensor,'LReLU1')\n",
    "        pool1 = tf.layers.max_pooling1d(activ1,pool_size=2,strides=2, name = 'maxPool1')    \n",
    "        \n",
    "        w = graph.get_tensor_by_name('siameseNet/Conv2_w:0').eval()\n",
    "        b = graph.get_tensor_by_name('siameseNet/Conv2_b:0').eval()\n",
    "        wTensor = tf.get_variable('Conv2_w',initializer=tf.constant(w))\n",
    "        bTensor = tf.get_variable('Conv2_b',initializer=tf.constant(b))\n",
    "        c2 = Conv1DWithWeights(pool1,wTensor,bTensor,64,'Conv2')\n",
    "        \n",
    "        offset = graph.get_tensor_by_name('siameseNet/BN2_offset:0').eval()\n",
    "        scale = graph.get_tensor_by_name('siameseNet/BN2_scale:0').eval()\n",
    "        bn2 = batchNormWithWeights(c2,offset,scale,'BN2')\n",
    "        #activ2 = tf.nn.relu(bn2)\n",
    "        alpha = graph.get_tensor_by_name('siameseNet/LReLU2_w:0').eval()\n",
    "        alphaTensor = tf.get_variable('LReLU2_w',initializer=tf.constant(alpha))\n",
    "        activ2 = LeakyReLUWithWeights(bn2,alphaTensor,'LReLU2')\n",
    "        pool2 = tf.layers.max_pooling1d(activ2,pool_size=2,strides=2, name = 'maxPool1')    \n",
    "        \n",
    "        w = graph.get_tensor_by_name('siameseNet/Conv3_w:0').eval()\n",
    "        b = graph.get_tensor_by_name('siameseNet/Conv3_b:0').eval()\n",
    "        wTensor = tf.get_variable('Conv3_w',initializer=tf.constant(w))\n",
    "        bTensor = tf.get_variable('Conv3_b',initializer=tf.constant(b))\n",
    "        c3 = Conv1DWithWeights(pool2,wTensor,bTensor,96,'Conv3')\n",
    "        \n",
    "        offset = graph.get_tensor_by_name('siameseNet/BN3_offset:0').eval()\n",
    "        scale = graph.get_tensor_by_name('siameseNet/BN3_scale:0').eval()\n",
    "        bn3 = batchNormWithWeights(c3,offset,scale,'BN3')\n",
    "        #activ3 = tf.nn.relu(bn3)\n",
    "        alpha = graph.get_tensor_by_name('siameseNet/LReLU3_w:0').eval()\n",
    "        alphaTensor = tf.get_variable('LReLU3_w',initializer=tf.constant(alpha))\n",
    "        activ3 = LeakyReLUWithWeights(bn3,alphaTensor,'LReLU3')\n",
    "        \n",
    "        w = graph.get_tensor_by_name('siameseNet/Conv4_w:0').eval()\n",
    "        b = graph.get_tensor_by_name('siameseNet/Conv4_b:0').eval()\n",
    "        wTensor = tf.get_variable('Conv4_w',initializer=tf.constant(w))\n",
    "        bTensor = tf.get_variable('Conv4_b',initializer=tf.constant(b))\n",
    "        c4 = Conv1DWithWeights(activ3,wTensor,bTensor,64,'Conv4')\n",
    "        \n",
    "        w = graph.get_tensor_by_name('siameseNet/Conv5_w:0').eval()\n",
    "        b = graph.get_tensor_by_name('siameseNet/Conv5_b:0').eval()\n",
    "        wTensor = tf.get_variable('Conv5_w',initializer=tf.constant(w))\n",
    "        bTensor = tf.get_variable('Conv5_b',initializer=tf.constant(b))\n",
    "        c5 = Conv1DWithWeights(c4,wTensor,bTensor,64,'Conv5')\n",
    "        \n",
    "        pool3 = tf.layers.max_pooling1d(c5,pool_size=2,strides=2, name = 'maxPool3')    \n",
    "        \n",
    "        #flattenSize = pool3.shape[1]*pool3.shape[2]\n",
    "        flatten = tf.reshape(pool3,[-1,17*64],name='flatten')\n",
    "        # dense 32        \n",
    "        w = graph.get_tensor_by_name('siameseNet/FC_w:0').eval()\n",
    "        b = graph.get_tensor_by_name('siameseNet/FC_b:0').eval()\n",
    "        wTensor = tf.get_variable('FC_w',initializer=tf.constant(w))\n",
    "        bTensor = tf.get_variable('FC_b',initializer=tf.constant(b))\n",
    "        fc = FullyConnectedWithWeights(flatten,wTensor,bTensor,32,'FC')\n",
    "        return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDiffNN(tensor1,tensor2,modelPath):\n",
    "    with tf.name_scope('DiffNN'):\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.import_meta_graph(modelPath + '.meta')\n",
    "            saver.restore(sess,modelPath)\n",
    "            graph = tf.get_default_graph()  #to get default graph\n",
    "            \n",
    "            merged = tf.concat([tensor1,tensor2],1,'merge')        \n",
    "            \n",
    "            w = graph.get_tensor_by_name('DiffNNet/FC1_w:0').eval()\n",
    "            b = graph.get_tensor_by_name('DiffNNet/FC1_b:0').eval()\n",
    "            wTensor = tf.get_variable('FC1_w',initializer=tf.constant(w))\n",
    "            bTensor = tf.get_variable('FC1_b',initializer=tf.constant(b))\n",
    "            fc1 = FullyConnectedWithWeights(merged,wTensor,bTensor,32,'FC1')\n",
    "            \n",
    "            w = graph.get_tensor_by_name('DiffNNet/FC2_w:0').eval()\n",
    "            b = graph.get_tensor_by_name('DiffNNet/FC2_b:0').eval()\n",
    "            wTensor = tf.get_variable('FC2_w',initializer=tf.constant(w))\n",
    "            bTensor = tf.get_variable('FC2_b',initializer=tf.constant(b))\n",
    "            fc2 = FullyConnectedWithWeights(fc1,wTensor,bTensor,32,'FC2')\n",
    "            \n",
    "            w = graph.get_tensor_by_name('DiffNNet/predictions_w:0').eval()\n",
    "            b = graph.get_tensor_by_name('DiffNNet/predictions_b:0').eval()\n",
    "            wTensor = tf.get_variable('predictions_w',initializer=tf.constant(w))\n",
    "            bTensor = tf.get_variable('predictions_b',initializer=tf.constant(b))\n",
    "            predictions = FullyConnectedWithWeights(fc2,wTensor,bTensor,2,'predictions')\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the FCNet\n",
    "### Code for Individual blocks of the FCNet is complete. Now we need to build a complete FCNet and train it on the data! The data for training contains pair of time series signals and labels as 0 and or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainModel(nEpochs=10):    \n",
    "\n",
    "    x1 = tf.placeholder(tf.float32, shape=[None, 172,1])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "    \n",
    "    x2 = tf.placeholder(tf.float32, shape=[None, 172,1])\n",
    "    # x1 and x2 are time series signals and y is the label for each pair of signals\n",
    "    \n",
    "    # create a pair of rows with sharing variable architecture \n",
    "    \n",
    "    with tf.name_scope('siameseNet_scope'):\n",
    "        with tf.variable_scope(\"siameseNet\") as scope:\n",
    "            p1 = makeRow(x1)\n",
    "            scope.reuse_variables()\n",
    "            p2 = makeRow(x2)\n",
    "    \n",
    "    # Similarity net \n",
    "    \n",
    "    with tf.name_scope('DiffNet_scope'):\n",
    "        with tf.variable_scope(\"DiffNNet\") as scope:\n",
    "            predictions = makeDiffNN(p1,p2) \n",
    "    \n",
    "    # loss and accuracy definations\n",
    "    \n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=predictions),name='crossEnt')\n",
    "    optimizer = tf.train.AdamOptimizer()  #optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "    with tf.name_scope('train'):\n",
    "        train_step = optimizer.minimize(cross_entropy,name='train')\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(predictions, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32),name='accuracy')\n",
    "    \n",
    "    # summaries for tensorboard\n",
    "    \n",
    "    with tf.name_scope(\"sumaries\"):\n",
    "        \n",
    "        tf.summary.scalar(\"accuracy\",accuracy)\n",
    "        tf.summary.scalar('Loss',cross_entropy)\n",
    "        tf.summary.histogram('histogram_loss',cross_entropy)\n",
    "        tf.summary.histogram('histogram_accuracy',accuracy)\n",
    "        summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    # At this point, model is full ycreated. \n",
    "    \n",
    "    # optional - load data\n",
    "    signalSrc = loadMatlabVar('FCNetAugmentedData.mat','signalSrc')\n",
    "    signalDest = loadMatlabVar('FCNetAugmentedData.mat','signalDest')\n",
    "    signalLabel = loadMatlabVar('FCNetAugmentedData.mat','signalLabel')\n",
    "    \n",
    "    # process the data now\n",
    "    trainLabelHV = np.array( list(map(lambda x: (0,1) if x==1 else (1,0),signalLabel)))\n",
    "    # soem hyper params\n",
    "    nSamples = trainLabelHV.shape[0]\n",
    "    \n",
    "    batch_size = 100\n",
    "    nBatches = int(nSamples / batch_size)\n",
    "    \n",
    "    trainCol1 = signalSrc.reshape(signalSrc.shape[0],signalSrc.shape[1],1)\n",
    "    trainCol2 = signalDest.reshape(signalDest.shape[0],signalDest.shape[1],1)\n",
    "    \n",
    "    with tf.Session(config = config) as sess:\n",
    "      # init all vars  \n",
    "      sess.run(tf.global_variables_initializer())\n",
    "      saver = tf.train.Saver()\n",
    "      \n",
    "      writer = tf.summary.FileWriter(summaryPath,graph=tf.get_default_graph())\n",
    "      for e in range(nEpochs):\n",
    "          file = open(\"results e{}.txt\".format(e),\"w\") \n",
    "          loss=0\n",
    "          train_accuracy =0\n",
    "          \n",
    "          for batchNumber in range (0,nBatches):\n",
    "              #bid*batch_size:(bid+1)*batch_size\n",
    "              startInd = batchNumber * batch_size\n",
    "              endInd = (batchNumber + 1) * batch_size\n",
    "        \n",
    "              _, crossLoss,acc,summary = sess.run([train_step,cross_entropy,accuracy,summary_op], feed_dict={x1: trainCol1[startInd:endInd,:], \n",
    "                                        x2: trainCol2[startInd:endInd,:], \n",
    "                                        y: trainLabelHV[startInd:endInd,:]\n",
    "                                        })\n",
    "              loss += crossLoss\n",
    "              train_accuracy += acc\n",
    "              file.write(\"e {}, batch id {} accuracy {} loss {} \\n\".format(e,batchNumber,acc,crossLoss))\n",
    "              \n",
    "          print('epoch %d of %d, loss %g training accuracy %g' % (e, nEpochs ,loss/nBatches,train_accuracy/nBatches))\n",
    "          writer.add_summary(summary,e)\n",
    "          file.close()\n",
    "      save_path = saver.save(sess, modelPath)\n",
    "      print(\"Model saved in file: %s\" % save_path)\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
